{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla_Seq_to_Seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPe9yzEMd82h"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcrMP9zeeI-c"
      },
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BFybn0eLGH"
      },
      "source": [
        "%%capture\n",
        "!tar -xvf  'daksh.tar' "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juf8qHq0fvy3"
      },
      "source": [
        "def data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if input_tokenizer is None:\n",
        "      df=df.sample(frac=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  #only train set will have input_tokenizer as none. Validation and test will will use the same.\n",
        "  if input_tokenizer is None:\n",
        "    input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "  input_tensor = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,padding='post')\n",
        "  if target_tokenizer is None:\n",
        "    target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "  #tokenize the text\n",
        "  target_tensor = target_tokenizer.texts_to_sequences(target_texts)\n",
        "  #pad the text\n",
        "  target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post')\n",
        "  #for dataset which is not training we pad to make maximum length same as train set.\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYZoLar-s1uh"
      },
      "source": [
        "%%capture\n",
        "input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jxJdIvkttoM"
      },
      "source": [
        "%%capture\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5LiYS3jQGXn"
      },
      "source": [
        "%%capture\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8s1F9bzYUNd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxf7x1eVjiFW"
      },
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  input_tensor.shape[1]\n",
        "max_decoder_seq_length = target_tensor.shape[1]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GryfWXxJyNgl"
      },
      "source": [
        "\n",
        "#convert index to character\n",
        "index_to_char_input = dict((input_tokenizer.word_index[key], key) for key in input_tokenizer.word_index.keys())\n",
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "019RKVafn4tS"
      },
      "source": [
        "\n",
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout):\n",
        "  #input layer ; takes in tokenize input\n",
        "  encoder_inputs = keras.Input(shape=( max_encoder_seq_length))\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "  #will store output of last added layer so that we can add multiple layers\n",
        "  last_encoder=None\n",
        "  if rnn_type=='LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.LSTM(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last LSTM Layer\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one LSTM layer.The above loop won't run.\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state_h, state_c = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state_h, state_c = encoder(last_encoder)\n",
        "    #storing the hidden states only\n",
        "    encoder_states = [state_h, state_c]\n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.GRU(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last GRU Layer\n",
        "    encoder = keras.layers.GRU(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one GRU layer.The above loop won't run\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(encoder_layers-1):      \n",
        "      encoder = keras.layers.SimpleRNN(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last RNN Layer\n",
        "    encoder = keras.layers.SimpleRNN(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one RNN layer.The above loop won't run\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]  \n",
        "\n",
        "\n",
        "  decoder_inputs = keras.Input(shape=( max_decoder_seq_length))\n",
        "  embed = keras.layers.Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
        "\n",
        "  if rnn_type==\"LSTM\":\n",
        "    #add all the LSTM layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _, _ = decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  elif rnn_type==\"GRU\":\n",
        "    #add all the GRU layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  elif rnn_type==\"RNN\":\n",
        "    #add all the RNN layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.SimpleRNN(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  #specifying model inputs and outputs.\n",
        "  # encoder_inputs -> Input to encoder\n",
        "  # decoder_inputs -> Input to decoder for teacher forcing\n",
        "  # decoder_outputs -> Output\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaYf9aa9yKxm"
      },
      "source": [
        "import copy\n",
        "def build_inference(model,encoder_layers,decoder_layers):\n",
        "    encoder_inputs = model.input[0]  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "    decoder_inputs =  keras.Input(shape=( 1))  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        #every layer must have an input through which we can supply it's hidden state\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "        x = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input_h)\n",
        "        decoder_states_inputs.append (decoder_state_input_c)\n",
        "        decoder_states.append (state_h_dec)\n",
        "        decoder_states.append (state_c_dec)\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      decoder_states_inputs=[] #Contain all input layers for different GRU's hidden state\n",
        "      decoder_states=[] #Contains the hidden states\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)      \n",
        "    decoder_dense = model.get_layer('final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "    return encoder_model,decoder_model\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBRaLSUWMepJ"
      },
      "source": [
        "def decode_batch(input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      states_value=[states_value]\n",
        "    nl=states_value\n",
        "    for i in range(decoder_layers-1):\n",
        "      nl=nl+states_value\n",
        "    states_value=nl\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        out = decoder_model.predict(tuple([prev_char_index] + states_value))\n",
        "        output_probability=out[0]\n",
        "        states_value = out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENyYG1Z-y6RU"
      },
      "source": [
        "def test_accuracy(encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #Get all the predicted words\n",
        "  pred=decode_batch(test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  return float(success)/float(test_input_tensor.shape[0])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX3KGZbf5Hb2"
      },
      "source": [
        "def batch_validate(encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #get all the predicted words\n",
        "  pred=decode_batch(val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=val_target_texts[seq_index][1:-1]\n",
        "      #test the words one by one\n",
        "      if predicted_word == target_word:\n",
        "        success+=1\n",
        "  return float(success)/float(val_input_tensor.shape[0])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5zKzPicx-9x"
      },
      "source": [
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "def train():\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "  wandb.init()\n",
        "  rnn_type=wandb.config.rnn_type\n",
        "  embedding_dim=wandb.config.embedding_dim\n",
        "  latent_dim=wandb.config.latent_dim\n",
        "  enc_layer=wandb.config.enc_layer\n",
        "  dec_layer=wandb.config.dec_layer\n",
        "  dropout=wandb.config.dropout\n",
        "  epochs=wandb.config.epochs\n",
        "  bs=wandb.config.bs\n",
        "  wandb.run.name = 'epochs_'+str(epochs)+'_bs_'+str(bs)+'_rnn_type_'+str(rnn_type)+'_em_'+str(embedding_dim)+'_latd_'+str(latent_dim)+'_encs_'+str(enc_layer)+'_decs_'+str(dec_layer)+'_dr_'+str(dropout)\n",
        "\n",
        "\n",
        "  model=build_model(rnn_type=rnn_type,embedding_dim=embedding_dim,encoder_layers=enc_layer,decoder_layers=dec_layer,dropout=.1)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  for i in range(epochs):\n",
        "    hist=model.fit(\n",
        "        [input_tensor, target_tensor],\n",
        "        tf.concat([target_tensor[:,1:],tf.zeros((target_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=1,shuffle=True\n",
        "    )\n",
        "    # Save model\n",
        "    model.save(\"s2s.keras\")\n",
        "    # Run inferencing\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "    encoder_model,decoder_model=build_inference(inf,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "    #log train loss to wandb\n",
        "    wandb.log({\"train_loss\": hist.history['loss'][0]})\n",
        "  val_acc=batch_validate(encoder_model,decoder_model,enc_layer,dec_layer)\n",
        "  wandb.log({\"val_acc\":val_acc})\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP1dYLLlXC6y"
      },
      "source": [
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "#this function is needed for training manually\n",
        "def manual_train(config):\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "  rnn_type=config.rnn_type\n",
        "  embedding_dim=config.embedding_dim\n",
        "  latent_dim=config.latent_dim\n",
        "  enc_layer=config.enc_layer\n",
        "  dec_layer=config.dec_layer\n",
        "  dropout=config.dropout\n",
        "  epochs=config.epochs\n",
        "  bs=config.bs\n",
        "  \n",
        "  model=build_model(rnn_type=rnn_type,embedding_dim=embedding_dim,encoder_layers=enc_layer,decoder_layers=dec_layer,dropout=.1)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=True,show_layer_names=True, dpi=96 )\n",
        "  for i in range(epochs):\n",
        "    hist=model.fit(\n",
        "        [input_tensor, target_tensor],\n",
        "        tf.concat([target_tensor[:,1:],tf.zeros((target_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=1,shuffle=True\n",
        "    )\n",
        "\n",
        "    model.save(\"s2s.keras\")\n",
        "\n",
        "    inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "    encoder_model,decoder_model=build_inference(inf,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "\n",
        "    val_acc=batch_validate(encoder_model,decoder_model,enc_layer,dec_layer)\n",
        "    print(\"Validation Accuracy\",val_acc)\n",
        "  print(\"Test Accuracy\",test_accuracy(encoder_model,decoder_model,enc_layer,dec_layer))    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKGZdYoS0Y0U"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHETIbqEYyXC"
      },
      "source": [
        "wb=False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4nsDBkF1opn"
      },
      "source": [
        "import wandb\n",
        "if wb:\n",
        "  wandb.login()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZOcPlID0Qkj"
      },
      "source": [
        "if wb:\n",
        "  wandb.agent('utsavdey/seq_to_seq/cdgdxg9i', function=train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGmGY2dYUb13"
      },
      "source": [
        "class configuration:\n",
        "  def __init__(self, rnn_type, embedding_dim,latent_dim,enc_layer,dec_layer,dropout,epochs,bs):\n",
        "    self.rnn_type = rnn_type\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.enc_layer = enc_layer\n",
        "    self.dec_layer = dec_layer\n",
        "    self.dropout = dropout\n",
        "    self.epochs = epochs\n",
        "    self.bs = bs\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_8Q5HtKYa8m"
      },
      "source": [
        "if not wb:\n",
        "  config=configuration('LSTM',32,512,3,2,.3,20,64)\n",
        "  manual_train(config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}