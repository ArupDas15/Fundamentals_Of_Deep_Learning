{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "seq2seq_with_attention (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:52:49.095997Z",
          "iopub.execute_input": "2021-05-20T10:52:49.096386Z",
          "iopub.status.idle": "2021-05-20T10:53:31.938155Z",
          "shell.execute_reply.started": "2021-05-20T10:52:49.096309Z",
          "shell.execute_reply": "2021-05-20T10:53:31.937318Z"
        },
        "trusted": true,
        "id": "lwO8HXVRQtjc",
        "cellView": "form"
      },
      "source": [
        "#@title Code to Import the required Libraries.\n",
        "!pip install uniseg\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "# Imports for visualisations\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpqgdQjP1oeX",
        "cellView": "form"
      },
      "source": [
        "#@title Code to import wandb and login to wandb (Run this cell if you wish to use wandb)\n",
        "!pip install wandb --upgrade\n",
        "import wandb\n",
        "!wandb login --relogin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaViDRb5lk5B"
      },
      "source": [
        "# How to debug the code?\n",
        "\n",
        "Reduce the number of word pairs being generated by create_dataset() to some small value say 100. However reduce the batch size to a value less than 100 else you may encounter [StopIteration() error](https://stackoverflow.com/questions/48709839/stopiteration-generator-output-nextoutput-generator).\n",
        "\n",
        "Also, to reduce the validation datasize reduce the number of inputs in validate() to some small number like 10 instead of len(input_words). \n",
        "\n",
        "Run the train() for 1 epoch. To visualise the code for 20 epochs you can restore the model parameters as provided in the [github repository](https://github.com/utsavdey/cs6910_assignment3/blob/main/seq2seq_with_attention/training_checkpoints.zip) using `checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))` after runnning the train function for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:53:31.941095Z",
          "iopub.execute_input": "2021-05-20T10:53:31.941361Z",
          "iopub.status.idle": "2021-05-20T10:53:31.947132Z",
          "shell.execute_reply.started": "2021-05-20T10:53:31.941330Z",
          "shell.execute_reply": "2021-05-20T10:53:31.946333Z"
        },
        "trusted": true,
        "id": "Lrj1MqA2Qtjg"
      },
      "source": [
        "# Common Error: https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
        "# https://pgaleone.eu/tensorflow/tf.function/2019/03/21/dissecting-tf-function-part-1/\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejr8zQ9IUzSi"
      },
      "source": [
        "# **NOTE:** \n",
        "The Hindi font file for displaying Hindi characters in the matplotlib plots [here](https://drive.google.com/file/d/11B4BahRBIujMr_jhsw_uXbxN9LF5CHaX/view?usp=sharing). A copy of the same has been upload in the GitHub project repository. *Kindly upload the same before generating the heatmaps.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:53:31.948920Z",
          "iopub.execute_input": "2021-05-20T10:53:31.949260Z",
          "iopub.status.idle": "2021-05-20T10:54:00.445148Z",
          "shell.execute_reply.started": "2021-05-20T10:53:31.949225Z",
          "shell.execute_reply": "2021-05-20T10:54:00.444235Z"
        },
        "trusted": true,
        "id": "jcJZCFpLQtjh",
        "cellView": "form"
      },
      "source": [
        "#@title Download Dataset and set file paths\n",
        "# Download the dataset\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "# Extract the downloaded tar file\n",
        "!tar -xvf  'daksh.tar'\n",
        "# Set the file paths to train, validation and test dataset\n",
        "train_file_path=os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.train.tsv\")\n",
        "vaildation_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.dev.tsv\")\n",
        "test_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.test.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:00.455488Z",
          "iopub.execute_input": "2021-05-20T10:54:00.456015Z",
          "iopub.status.idle": "2021-05-20T10:54:13.245716Z",
          "shell.execute_reply.started": "2021-05-20T10:54:00.455980Z",
          "shell.execute_reply": "2021-05-20T10:54:13.244718Z"
        },
        "trusted": true,
        "id": "A-rEFwEUQtji",
        "cellView": "form"
      },
      "source": [
        "#@title Data pre-processing and tokenisation \n",
        "def preprocess_word(w):\n",
        "  w = '\\t' + w + '\\n'\n",
        "  return w\n",
        "\n",
        "# Returns pairs of target word,input word. E.g. [['\\tअं\\n', '\\tan\\n'], ['\\tअंकगणित\\n', '\\tankganit\\n']]\n",
        "def create_dataset(path):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_word(w)  for w in line.split('\\t')[:-1]]\n",
        "                for line in lines[:-1]]\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path):\n",
        "  targ_lang, inp_lang = create_dataset(path)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13mCNK7NWfb1"
      },
      "source": [
        "The above code segment is responsible for the below tasks:\n",
        "\n",
        "*   Returns pairs of target word,input word: [HINDI, ENGLISH] as a list.\n",
        "*   Every target word and input word is appended with a Start of Sequence Character '\\t' and End of Sequence Character '\\n'.\n",
        "*   Finally, we make a dictionary of all the unique characters in the training dataset for both the input language(ENGLISH: inp_lang) and the target language(HINDI: targ_lang).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:13.252248Z",
          "iopub.execute_input": "2021-05-20T10:54:13.253565Z",
          "iopub.status.idle": "2021-05-20T10:54:14.785207Z",
          "shell.execute_reply.started": "2021-05-20T10:54:13.253522Z",
          "shell.execute_reply": "2021-05-20T10:54:14.783750Z"
        },
        "trusted": true,
        "id": "SwTGFdWJQtjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "f41eca7a-bba5-417f-e66e-ced28616dbfc"
      },
      "source": [
        "#@title Create Input word tensors, target word tensors, input vocabulary and output vocabulary from training set.\n",
        "# Use the entire training dataset file\n",
        "input_tensor_train, target_tensor_train, inp_lang, targ_lang = load_dataset(train_file_path)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor_train.shape[1], input_tensor_train.shape[1]\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44203 44203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:14.786599Z",
          "iopub.execute_input": "2021-05-20T10:54:14.786929Z",
          "iopub.status.idle": "2021-05-20T10:54:14.801558Z",
          "shell.execute_reply.started": "2021-05-20T10:54:14.786893Z",
          "shell.execute_reply": "2021-05-20T10:54:14.800443Z"
        },
        "trusted": true,
        "id": "76sT_CuwQtjj",
        "cellView": "form"
      },
      "source": [
        "#@title Definition of RNN, LSTM and GRU Encoder\n",
        "class GRU_Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0):\n",
        "    super(GRU_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = dropout)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "\n",
        "class LSTM_Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz,dropout=0):\n",
        "    super(LSTM_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(self.enc_units, \n",
        "                         return_sequences=True, \n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "\n",
        "  def call(self, x, hidden,cell_state):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    output, last_hidden,last_cell_state = self.lstm(x, initial_state=[hidden,cell_state])\n",
        "    return output, last_hidden,last_cell_state\n",
        "    \n",
        "  def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "class RNN_Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz,dropout=0):\n",
        "    super(RNN_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(self.enc_units, \n",
        "                         return_sequences=True, \n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, final_state = self.rnn(x,initial_state=hidden)\n",
        "    return output, final_state\n",
        "    \n",
        "  def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unY8v3ssaQZO"
      },
      "source": [
        "Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio, [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "\n",
        "[Reference](https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e) to write your own custom attention layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:14.803869Z",
          "iopub.execute_input": "2021-05-20T10:54:14.804205Z",
          "iopub.status.idle": "2021-05-20T10:54:15.047718Z",
          "shell.execute_reply.started": "2021-05-20T10:54:14.804170Z",
          "shell.execute_reply": "2021-05-20T10:54:15.046429Z"
        },
        "trusted": true,
        "id": "X_-WfXK2Qtjm",
        "cellView": "form"
      },
      "source": [
        "#@title Definition of Attention Layer\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:15.053334Z",
          "iopub.execute_input": "2021-05-20T10:54:15.055474Z",
          "iopub.status.idle": "2021-05-20T10:54:15.090661Z",
          "shell.execute_reply.started": "2021-05-20T10:54:15.055428Z",
          "shell.execute_reply": "2021-05-20T10:54:15.089097Z"
        },
        "trusted": true,
        "id": "79tG6ZjLQtjn",
        "cellView": "form"
      },
      "source": [
        "#@title Definition for RNN, LSTM and GRU Decoder.\n",
        "class GRU_Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(GRU_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=dropout)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    \n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "class LSTM_Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(LSTM_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=dropout)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output,cell_state):\n",
        "    \n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, last_hidden_state,last_cell_state = self.lstm(x,initial_state=[hidden,cell_state])\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, [last_hidden_state,last_cell_state], attention_weights\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    \n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(self.dec_units, \n",
        "                         return_sequences=True, \n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, final_state = self.rnn(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, final_state, attention_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:15.096300Z",
          "iopub.execute_input": "2021-05-20T10:54:15.098586Z",
          "iopub.status.idle": "2021-05-20T10:54:15.106463Z",
          "shell.execute_reply.started": "2021-05-20T10:54:15.098542Z",
          "shell.execute_reply": "2021-05-20T10:54:15.105246Z"
        },
        "trusted": true,
        "id": "uqs5mEwTQtjo"
      },
      "source": [
        "# Reference: https://stackoverflow.com/questions/62916592/loss-function-for-sequences-in-tensorflow-2-0\n",
        "def calculate_loss(real, pred):\n",
        "  mask_position = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_value = loss_object(real, pred)\n",
        "\n",
        "  mask_position = tf.cast(mask_position, dtype=loss_value.dtype)\n",
        "  loss_value *= mask_position\n",
        "\n",
        "  return tf.reduce_mean(loss_value)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkG5UyLwuMHa"
      },
      "source": [
        "Set ```use_wandb=False``` while calling train function if you do not desire to log results into wandb. Currently, the hyperparameters have been set to the best configurations we obtained during pur experiments.\n",
        "\n",
        "We use a single layered encoder and a single layered decoder and add an attention network to this basic sequence to sequence model and train the model. \n",
        "\n",
        "Once the model taining is complete we find the validation accuracy and report the test accuracy and create a folder ***prediction_attention*** with a sub-folder having the name of the hyperparameter configuration(= **run_name**) where we create two files  `success.txt` and `failure.txt`. These files contain `<input word><space><target word><space><predicted word>` of the successful and failed predictions made by the sequence to sequence to sequence.\n",
        "\n",
        "We randomly choose 10 inputs from our test data and generate a 3*3 attention heatmaps on the same.\n",
        "\n",
        "We then perform a [connectivity visualisation](https://distill.pub/2019/memorization-in-rnns/#appendix-autocomplete) for the predictions made by our model. Currently, the connectivity visualisation can be performed on any three romanised words. For example: here we have chosen `doctor`, `prayogshala` and `angarakshak` for the purpose of visualisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:15.111756Z",
          "iopub.execute_input": "2021-05-20T10:54:15.117500Z",
          "iopub.status.idle": "2021-05-20T10:54:15.150307Z",
          "shell.execute_reply.started": "2021-05-20T10:54:15.117463Z",
          "shell.execute_reply": "2021-05-20T10:54:15.149572Z"
        },
        "trusted": true,
        "id": "ENGo0yHWQtjp"
      },
      "source": [
        "def train(use_wandb=True):\n",
        "    \n",
        "    global BATCH_SIZE \n",
        "    global units \n",
        "    global vocab_inp_size\n",
        "    global vocab_tar_size\n",
        "    global embedding_dim\n",
        "    global encoder\n",
        "    global decoder\n",
        "    global optimizer\n",
        "    global loss_object\n",
        "    global checkpoint_dir\n",
        "    global checkpoint_prefix \n",
        "    global checkpoint\n",
        "    global run_name\n",
        "    global rnn_type\n",
        "\n",
        "    if use_wandb==True:\n",
        "      # initialising the wandb run\n",
        "      run = wandb.init()\n",
        "      # Tpye of RNN to choose. Acceptable Values are 'RNN'. 'LSTM' and 'GRU'\n",
        "      rnn_type = run.config.rnn_type\n",
        "      # Batch size for training.\n",
        "      BATCH_SIZE = run.config.bs\n",
        "      # Dimensions of the abstract representation of the input word and target word.\n",
        "      embedding_dim = run.config.embed\n",
        "      # Latent dimensions of the encoder and decoder.\n",
        "      units = run.config.latent\n",
        "      # Number of epochs to train for.\n",
        "      EPOCHS = run.config.epochs\n",
        "      #\tFloat between 0 and 1. Denotes the fraction of the units to drop.\n",
        "      dropout = run.config.dropout\n",
        "    else:\n",
        "      rnn_type = 'LSTM'\n",
        "      BATCH_SIZE = 64\n",
        "      embedding_dim = 512\n",
        "      units = 1024\n",
        "      EPOCHS = 20\n",
        "      dropout = 0.2\n",
        "\n",
        "    print(\"rnn_Type: \",rnn_type)\n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "    vocab_inp_size = len(inp_lang.word_index)+1\n",
        "    vocab_tar_size = len(targ_lang.word_index)+1\n",
        "    \n",
        "    run_name = '_epochs_'+str(EPOCHS)+'_rnn_type_'+str(rnn_type)+'_bs_'+str(BATCH_SIZE)+'_embed_'+str(embedding_dim)+'_latent_'+str(units)+'_dropout_'+str(dropout)\n",
        "    if use_wandb==True:\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "    \"\"\" We are using Python iterable object called Dataset. \n",
        "    This makes it easier for us to consume its elements using an iterator. \n",
        "    We have created this dataset using an in-memory data. \n",
        "    The training datapoints are chosen uniformly at random.\"\"\" \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "    # We create batches of size BATCH_SIZE and ignore the last batch because the last batch may not be equal to BATCH_SIZE\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    \n",
        "    \"\"\" Build model\n",
        "    We are explicitly creating a Python iterator using iter and consuming its elements using next. \n",
        "    For Hindi: TensorShape([64, 22]), TensorShape([64, 21]) is the shape of train_input_batch and train_target_batch respectively.\"\"\"\n",
        "    train_input_batch, train_target_batch = next(iter(dataset))\n",
        "    \n",
        "    # sample input\n",
        "    if rnn_type=='GRU':\n",
        "       encoder = GRU_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "       sample_hidden = encoder.initialize_hidden_state()\n",
        "       sample_output, sample_hidden = encoder(train_input_batch, sample_hidden)\n",
        "    elif rnn_type=='LSTM':\n",
        "      encoder = LSTM_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_hidden,sample_cell_state = encoder.initialize_hidden_state()\n",
        "      sample_output, sample_hidden,sample_cell_state = encoder(train_input_batch, sample_hidden,sample_cell_state)\n",
        "    elif rnn_type=='RNN':\n",
        "      encoder = RNN_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_hidden = encoder.initialize_hidden_state()\n",
        "      sample_output, sample_hidden = encoder(train_input_batch, sample_hidden)\n",
        "    print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "    print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n",
        "    \n",
        "    if rnn_type=='GRU':\n",
        "      decoder = GRU_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "    \n",
        "    elif rnn_type=='LSTM':\n",
        "      decoder = LSTM_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output, sample_cell_state)\n",
        "    \n",
        "    elif rnn_type=='RNN':\n",
        "      decoder = RNN_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "      \n",
        "    print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    \n",
        "    checkpoint_dir = os.path.join(os.getcwd(),'training_checkpoints')\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "    \n",
        "    train_loss=[0]*EPOCHS\n",
        "    \n",
        "    # Start training\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      if rnn_type!='LSTM':\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "      elif rnn_type=='LSTM':\n",
        "        enc_hidden,enc_cell_state = encoder.initialize_hidden_state()\n",
        "      total_loss = 0\n",
        "      for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        if rnn_type!='LSTM':\n",
        "          batch_loss = train_every_step(inp, targ, enc_hidden, encoder,decoder,rnn_type)\n",
        "        elif rnn_type=='LSTM':\n",
        "          batch_loss = train_every_step(inp, targ, [enc_hidden,enc_cell_state], encoder,decoder,rnn_type)\n",
        "        total_loss += batch_loss\n",
        "      if batch % 100 == 0:\n",
        "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "        \n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "      print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "      print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "      # Storing the average loss per epoch\n",
        "      train_loss[epoch] = total_loss.numpy()/steps_per_epoch\n",
        "      if use_wandb == True:\n",
        "        wandb.log({\"train_loss\": total_loss.numpy()/steps_per_epoch})\n",
        "\n",
        "        \n",
        "    test_accuracy = validate(test_file_path,run_name)\n",
        "    val_acc=validate(vaildation_file_path,rnn_type)\n",
        "    print(\"Train loss: \",train_loss)\n",
        "    print(\"Validation Accuracy: \",val_acc)\n",
        "    print(\"Test Accuracy: \",test_accuracy)\n",
        "    if use_wandb ==True:\n",
        "      wandb.log({'val_accuracy': val_acc})\n",
        "    \n",
        " \t  # restoring the latest checkpoint in checkpoint_dir and starting the test\n",
        "  \t# checkpoints are only useful when source code that will use the saved parameter values is available.\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    generate_inputs(rnn_type,10)\n",
        "    connectivity(['doctor','prayogshala','angarakshak'],rnn_type, os.path.join(os.getcwd(),\"predictions_attention\",str(run_name)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:15.153163Z",
          "iopub.execute_input": "2021-05-20T10:54:15.153421Z",
          "iopub.status.idle": "2021-05-20T10:54:17.977808Z",
          "shell.execute_reply.started": "2021-05-20T10:54:15.153379Z",
          "shell.execute_reply": "2021-05-20T10:54:17.976767Z"
        },
        "trusted": true,
        "id": "BlO69GsyQtjq"
      },
      "source": [
        "@tf.function\n",
        "def train_every_step(inp, targ, enc_hidden, enocder, decoder,rnn_type):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "        if rnn_type!='LSTM':\n",
        "            enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "        elif rnn_type=='LSTM':\n",
        "            enc_output, enc_hidden,enc_cell_state = encoder(inp, enc_hidden[0],enc_hidden[1])\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_cell_state=enc_cell_state\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['\\t']] * BATCH_SIZE, 1)\n",
        "        \n",
        "        # Teacher forcing - passing the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            if rnn_type!='LSTM':\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            elif rnn_type=='LSTM':\n",
        "                if t==1:\n",
        "                  # passing enc_output to the decoder\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output,dec_cell_state)\n",
        "                elif t>1:\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden[0], enc_output,dec_cell_state)\n",
        "            loss += calculate_loss(targ[:, t], predictions)\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aZHpKjEf-b6"
      },
      "source": [
        "The inference_model() is similar to the train_every_step(). Here the only difference is that we don't use [teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). \n",
        "\n",
        "*   The decoder input at every time step is the prediction that it had made previously along with the hidden state and the encoder output. \n",
        "*   We stop predicting the target word when the model predicts the end token '\\n'.\n",
        "*   The attention weights obtained at every time step is stored and returned so that it can be used for heatmap generation, visualisation and connectivity exploration.\n",
        "\n",
        "It takes a transliterated romanized word as input and produces the corresponding indic language word here 'Hindi'.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "input_word: Accepts string as input. Here we pass the transliterated roman word.\n",
        "\n",
        "rnn_type: Accepts string as input. Here we pass the type of RNN being used. The acceptable values are 'RNN', 'LSTM', and 'GRU'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:17.979145Z",
          "iopub.execute_input": "2021-05-20T10:54:17.979537Z",
          "iopub.status.idle": "2021-05-20T10:54:17.994690Z",
          "shell.execute_reply.started": "2021-05-20T10:54:17.979489Z",
          "shell.execute_reply": "2021-05-20T10:54:17.993910Z"
        },
        "trusted": true,
        "id": "7r6oz02nQtjr"
      },
      "source": [
        "#@title Code for inference model.\n",
        "def inference_model(input_word,rnn_type):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  input_word = preprocess_word(input_word)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in input_word]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  predicted_word = ''\n",
        "  \n",
        "  if rnn_type!='LSTM':\n",
        "    \n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "  elif rnn_type=='LSTM':\n",
        "    hidden=tf.zeros((1, units))\n",
        "    cell_state= tf.zeros((1, units)) \n",
        "    enc_out, enc_hidden,enc_cell_state = encoder(inputs, hidden,cell_state)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['\\t']], 0)\n",
        "\n",
        "  att_w=[]\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    if rnn_type!='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n",
        "    elif rnn_type=='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out, enc_cell_state)\n",
        "      dec_hidden=dec_hidden[0]\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    att_w.append(attention_weights.numpy()[0:len(input_word)])\n",
        "    \n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    predicted_word += targ_lang.index_word[predicted_id] \n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '\\n':\n",
        "      return predicted_word, input_word, attention_plot,att_w\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return predicted_word, input_word, attention_plot,att_w"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbhlSjzdzNw9"
      },
      "source": [
        "`validate(path_to_file,folder_name)`: Finds the accuracy of the model on the test and validation dataset. \n",
        "\n",
        "It creates a folder ***prediction_attention*** with a sub-folder folder_name where we create two files  `success.txt` and `failure.txt`. These files contain `<input word><space><target word><space><predicted word>` of the successful and failed predictions made by the sequence to sequence to sequence.\n",
        "\n",
        "The parameters in validate() are the following:</br>\n",
        "**path_to_file**: Accepts parameters of type string. Contains the path to validation or test dataset. from  from the folder dakshina_dataset_v1.0/hi/lexicons/</br>\n",
        "**folder_name**: Accepts parameters of type string. This parameter is helpful in creating subfolder inside the prediction_attention folder as per the hyperparamter configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:17.996005Z",
          "iopub.execute_input": "2021-05-20T10:54:17.996352Z",
          "iopub.status.idle": "2021-05-20T10:54:18.009676Z",
          "shell.execute_reply.started": "2021-05-20T10:54:17.996318Z",
          "shell.execute_reply": "2021-05-20T10:54:18.008751Z"
        },
        "trusted": true,
        "id": "vdV2sQJdQtjr",
        "cellView": "form"
      },
      "source": [
        "#@title Function to find the accuracy of the model on the validation and test dataset.\n",
        "import shutil\n",
        "def validate(path_to_file,folder_name):\n",
        "  save = False\n",
        "  if path_to_file.find(\"test\")!=-1:\n",
        "    if os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name))):\n",
        "      shutil.rmtree(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "      \n",
        "    if not os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\")):\n",
        "        os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\"))\n",
        "    os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "    success_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"success.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    failure_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"failure.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    save=True\n",
        "    \n",
        "  success_count=0\n",
        "  # Get the target words and input words for the validation\n",
        "  target_words, input_words = create_dataset(path_to_file)\n",
        "  for i in range(len(input_words)):\n",
        "    predicted_word, input_word, attention_plot,att_w = inference_model(input_words[i],rnn_type)\n",
        "    record= input_word.strip()+' '+target_words[i].strip()+' '+predicted_word[:-1].strip()+\"\\n\"\n",
        "    # The last character of target_words[i] and predicted word is '\\n', first character of target_words[i] is '\\t'\n",
        "    if target_words[i][1:]==predicted_word:\n",
        "      success_count = success_count + 1\n",
        "      if save == True:\n",
        "        success_file.write(record)\n",
        "    elif save==True:\n",
        "      failure_file.write(record)\n",
        "\n",
        "  if save==True:\n",
        "    success_file.close()\n",
        "    failure_file.close()\n",
        "    \n",
        "  return success_count/len(input_words)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSuRBUvKRweo"
      },
      "source": [
        "`plot_attention()`: Responsible for the attention heatmap creation.\n",
        "\n",
        "#### Parameters\n",
        "**attention**: Accepts a list of attention weights learnt during the model inference for the test input.\n",
        "\n",
        "**input_word**: Accepts string as input. Here we pass the input word (English word).\n",
        "\n",
        "**predicted_word**: Accepts a string as input. Here we pass the prediction made by inference model.\n",
        "\n",
        "**file_name**: Accepts a string as input. Here we pass the path of the file where we want to store the heatmap.\n",
        "\n",
        "Below is a sample heatmap generated by the code:\n",
        "\n",
        "![_nazarandaaz_.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:22.461536Z",
          "iopub.execute_input": "2021-05-20T10:54:22.461864Z",
          "iopub.status.idle": "2021-05-20T10:54:22.470447Z",
          "shell.execute_reply.started": "2021-05-20T10:54:22.461833Z",
          "shell.execute_reply": "2021-05-20T10:54:22.469589Z"
        },
        "trusted": true,
        "id": "jt06vaPDQtjt",
        "cellView": "form"
      },
      "source": [
        "#@title Function to generate the attention heatmap for a transliterated roman word.\n",
        "def plot_attention(attention, input_word, predicted_word, file_name):\n",
        "  hindi_font = FontProperties(fname = os.path.join(os.getcwd(),\"Nirmala.ttf\"))\n",
        "  \n",
        "  fig = plt.figure(figsize=(3, 3))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "  \n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + list(input_word), fontdict=fontdict, rotation=0)\n",
        "  ax.set_yticklabels([''] + list(predicted_word), fontdict=fontdict,fontproperties=hindi_font)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.savefig(file_name)\n",
        "  plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2htk2GM63pl"
      },
      "source": [
        "`visualize()`: Helps to visualise what the sequence to sequence model learns with the help of attention network.\n",
        "#### Parameters:\n",
        "**input_word**: Accepts string as input. Here we pass the transliterated roman word\n",
        "\n",
        "**output_word**: Accepts string as input. Here we pass the predicted output word\n",
        "\n",
        "**att_w**: Takes a list of list where each sublist denotes the attention weights learnt at a particular timestep. Each of the sublist of size equal to length of the transliterated roman word which is fed as input.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "![Output_without_heatmap.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAP8AAAHpCAYAAABN8Or4AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsIAAA7CARUoSoAAADVySURBVHhe7d19bBznfSfwb4rCvX+EREnW0VmWZG4XQg9Mo2CjhoDil2VPRmj+EcX1Yk9xUcSkKKk2ZKHApfIe7cixE1IrVn8cLEE+KRTPQlHHXFFRWBwUBlLKtS3rQIskrCbEtcJma5GyrXBry4YOuMY43N7zzDwz88zsC3fJpXbJ5/sBxp7ZeZ/Z3zy/5xlqn88UBBCRcX5P/Z+IDMPgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyVJ2Cfxr94TDCgaF/Us1ebjfT6A73i6Og1WA6FUb3SF5N0XKpU/BH0ZvLIZcbQY+Y6knL8Rx6t9pziaj53Jm0f7If4dQ08iPdblbgPdnzSHd3I31Tzx70Ulx+LuerScErGdQ625LIYBBxd33/8pJcx79dqkS/V0XX08q0nHnOIK+tvJfBUrv4s3Lbdj6PnwQyB9q8ZcR3h5aB/Fd99TNV6GtpKfRdUZOOK32FFvF5y6EpbbpPLC3NF4a7xLyWrsLwB9YHhalD2rLWNr15kpzfdWZeTQkfDBe63O2VZm1zgWVIEdezT7u+82e6Ci1dw+JOWVPW/XKvv3UvvfvjX1aw7o12/ypu21Z0f2lZ3LkGv1gKE8moPb61Q1QPsshqpUlPegiJdfZ4dLuoPFzLijKjfqJJWRXpFRUUWtC6BHrjITUBhO7vQCyTxZycuDmOsUwMHfer+da9zCB7w54MxfehJzOGcac0vzSGzJ597r2tuG26o5q3tZ9fiAayU3U37baqVcq6CCJiauySejRPjokKVw863PadKDr2OPPzGD8PpJ7QH7kVtk13VPMGfyyCDWqU7qzpVBuSEJma1YgrhsspxNQ8iLsSERNunTwhQj/tz6hk5pY5P468zBLQgXan1Bcqb5vupCYM/mn0iy9UrLMdXnLopZWyUUg2CPlYpdEgxiq8WmSDX402R9T1FyV1r1Y6T/7YH7xiKHqrs3W3WCKJZ3rHEHkqod1Hpdy2lQ3i6WI9PNQ0LY86Bb/TUh8XIQgMJuyUrpb3/M46chvZgQkMufXCKHrTPe78tuw+jOxRs1z+ZUq19lP1ok+I0vhkXF3LNmQ7tdJZBXabNc8b/Pc6hPZOEcCZiFYdsFXcthKKH/bvg639y6IJfsNP1gHFl+Ap/l3AiiBf2x6PYGJIK9HlZwlgRGtQlRmafFDnnEZeajrNW+enppR/N6vGPNMXRb6nt9HcTOOZA8GGPmo2DH6qSVFKLob4tZSdCTh//LMtiYj26paaE3+6m8hQLPmJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzVl8HtdcGucX4ZtZAcO1jGshl5/9O7Qy3dwYvdytPI6QGna45b9G5S87tr9eMiZX2HwLfMIHtHnlRq60yV7P6oy+PUvixpWUi8q8qKXuQDLreSDrOGi6LW62hpBj/qkNnZnm7X0yFQ/jdz3Em3tdbs4y6UjSPbq38kYUpfF56+r+UWDvFfaMmlx52TP17mf4+cll59AKiZ7vxbjegcrmppKfmtDzoavxe/sA2BdAkNy3+wB5o6xuzVfeb+/vxKO2+r8xO2z0E/2duQraK3B7gqvnhaZ9oeQeEo8eU6OqRTYexrbKZccAmlX2ZRHstd35gc74vS2Wa4U9a/vdsjpVBUS4rJl9I4m9NQ9sG7wgaYfdw3dSTs3UJ6L26Otb/sLXzP/l0CfJ9eV03pGFqyOBLK1WjIf57qV2K59rG1Iigvh9Y0YuC+V7rWThen78F3zwP3QjnvBfVc4blulayLniXX0bZS4ZvY9CX5/a6CuTdv5Dkw4BdnNLPR+kELxIbtkF7wCd4EszXfN5WBfp4pkpx0Lmyr0tbQU+q6oSelKX6GlpU/MkeYLw10tYrql0HVm3vpk6pCYPmTPLXwwXOhq6SoMf2BP+tcNLCvIaWc7utKfq31r6xeR++saFksW8+/b3pa7j+BxW9PecVej3LlUc836tPXmz3Rp5+Cs6xxb4LiF8vvVyfuqnV9Q2fO19+f7PjgWuNf2tDx29Vlg+fkzfdrxFJ9XxX07Sh53cFv2tHfv7e+4d6wlvvOCdR8qXbNS5DkHv5/udXCGUtvU70/gXvm+06WOdeHrtMiSXzwlZTfaA7t9/bJjz4jbu67sox3XstaTM39pDBg47KVhsqfXmOpSWzxpj53swchi03nZZXRmsetPY8y3bzujcbqHnn4l6T/u5VDmmslqTq/bU7E4svs7EMtkMaempR63SyzVK25WnysyjgZ0c13xXrtk3VV16rmuHR0xrwv2ULxXu96lz2tR1Pdkn3tNxb3ulz0GO9mrpB2X+G/HHiD7rv8KWqVyrVUKeQ2uHfNnC1s7vDp82VI9it0DQPKV4hzGx8oceop6RF5ITcFfvhttW892LQBl44ZqaJjLZvypbzUpSZWsupPeSWQtrIs2iLh7XGKQVYQ7qNw1K0p/a6hySNHkhK9PvTvVQFbVvY51oF0L8MSQ1kOzL3UX6fGBOn1RpMV+T5ZMFioRJLdpVZHJMfHNiyASfIi4528vK69nLLLAUa8T2xHf5GyNVZFFNvjligJ/ITHxsHDWdYZ6dMkdui+ixhZLlPyB4yrXOnonTadE0IjwnXCO6XJxP/aV2UHlrJsVD+479QBY/L0WGaV4yEFbf2KgtrOuKJA54Ua2pgfqksgH++UIjjkPNlHI9KS9Ls1dViYkR+xCKS4yUy9bKUdmKZmFM4SARab9tZHpbObAM6UbSaynlpcWygaVYINfRTJ9yiTxTKXXafdGRMo8hvHg/q0LLS5ymbcWGyIi5XRTZ/uLWeuXxb+NGrmtwSIL6K193y7rGtdTCJHN4ut5sfi6VbzXVYrcp77sohR8pqjkL7/viqw0exDH3O+JuKbHS1RdF7CkBj/njVXFB6Lz0HZe7ZV4QJQQTYrlT8Zreq18R4LfeurJ95rbvHTOa42NojctbouqUrRl9wWe9l4Lrd5y7p2kfGc9gogv1Qy09IqLfljWndz9O/PlhVavLd11vW37u6M+hsjlBVpcSyjq0rrK16PRJ2R91DmuNmQ7ayn5vWtmD3Y1zf2yuS3D8vWRKDGs6+J9od23DNbDzqkW+a+p82Vz9uHej4r3eiGyjhvzqpfbsugoUfKX23fl4w5+T0Rmtdlrb2k+Mi4CVYWKnPOr/sHLLrqJmph8vXksUty+Vg8MfqIml7+ZR2gdg5+I6uTO1PmJqOkw+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDP5GcXquUZNUPb0HJ9811HqtWZF9+d1hTRL85btoqor1W+fV/tBhjZZz201MBlgtvwR7J9l98dnDyObALzdbnVfW52fhV7smCH4Z+G0Y63R+q111NlHlr9ySyfLIXtN+6ptq0vjgl90oiXA/7P46aSjQjZL8GWr/76R7pZL6iWrfTzXLwVlePljkuP5T1nopvpRtVynQgWKwNJ1zfm66xDz3p6itofRx6sv4U139nMVQ9DANzFfZlrO98h2MSv51fcftZkraMiUyOTt1X3xGZa9vFxpOKW/13kTVkz/g2UhWx4dFnWzqHQ8GOigUijqhXKBDSb0TRH/HnEvZdhWszhj923c5HTU6x2JNa/sR+y3fUac6D2394vn6MQc7crSnK3XkWb6jz2CHl4FtW9dLv+bBfdvs46/tusp1Sm9HG8p0yErFGl7yl+6LbAMideylyevQUtQX9c4wl5XTI0yFjj5l/dTpJNTqUUbrb62Kjjr19YPzo0m9pxd/p5P5kWMYFOt62VYNbo5jzJepqY42Aj3oeNe8dIeXdr29ut5oHLIjk+xxfxZh3U9Vz3e6tabqNDz4re6sinphnUM2E0PkXjVZb8EgWhbyHJZSHw00gpboKizW2S4qSYrVFZQWTIHqht4Fmnzgel2B1Uj2b5fReiASQ3Fnmv4eY2Wg16XTCXGO+zb724OmLw4u/lwM1/DgtzraDJbEVu+5JXowrZc70lvr0rKXJXXUKevdVkeQal0xjIjS1yEfuEvilLT6sNgu1mskHyQTkWPaQ61n8d27G67xDX5FHW2KEq9X9tSqd6Do9d8uG6SKOvIMdPZZ3jT6RVD4Ssy6bTvI7lt+MLGE14RL6qhTy5xEFqCfl1VFOBmv+C68bAej1XSMWoWlNPjZfeQ7D57aqg7kaXzwi1sX7EBRtuB6aaLssNDfkadeitn8ywRbxr3P7Q4r67ntSuSXdGIgq70pKG7RL2dJHXXK9Fh22ex0lnk8gpR+XrKKoLrtdo4r2CJfvoPR4P2yB/5RzcqzyrvrkvVmEThP8Y8+iIKaoOQnokZg8BMZir30EhmKJT+RoRj8RIZi8BMZisFPZCgGP5GhGPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBj+RoRj8RIZi8BMZisFPZCgGP5GhGv5LPv/7/vvVWHVeOvSqGmsuvQ9sVGNEKwNLfiJDMfiJDMXgX7Q8rj6/CZkZNUm0wjD4iQzVHMH/aR5//tZbWOMO/4T0p2peKTNH8eyDm+zh+VFcPS3+f7pEr2/5Ubz64F5c1fuhstY9ilk16WeX5u62xfDqxVLda8nltiI9Dlx4stwyRM2t8cEvA//KNXz1K9/A7W/I4U9w6osfYte/5PFbtYiPDOgnj+Dhl6+j7w0xJGaRPqXmBYW2obV9DDNa9M9OHkHrwZ0obptXAR0+Z29XDMmDHZh58Tn/w8MSwpYXJpFoh3Ucj29nB9G08jQ++O8K4e9E0P/1GjWNu/DQ57+gxovdvnoeM7vOIdaqPmjdaQVhaSGEHxAB/OZl3Lamp5E71YHWLaWCVQa0CPrven2+rtnSCWc3RKtNE9X5b+NvVNofufah+qzYrffG0Lpe710/hLVhNVqCFcDjs7glJ/JzeL+9E+EFCupZWY2Qaf9j+8H2PFqtmiD4naD/R7yz+U+s1D+7uXzJXzMr9T+CnIhimTXggW1wk4wAJ+hP5F5CUqb+Z19iyU+rVuOD//YnePGLm5EVQf93obvUh+WtXS/S+Pfm1JSUx62cGi3JTv0vTI4i96aoJZRM+SW7SpA4K4L+hR1lHxBEq0VzpP3/+jtcV6O4faNi2m+l8ace9d6vz7xmtbpXYq+zH2kslPKP4da8GhUPgwzTflrFGh/8a+7FLzdex390XvO9/wfIfmWTeCB8iNdLve4L7cDjL3/PesVm1cvTG5HYJUp0XztAgJX6i2UqpPxAFDF9uw+ewNqz5/CweCDobws8KqOQy5d6zUjU5FbFP+yRdfVL6ycrvHKTr/GeA/adQNmsf4n4D3topWmi1v7q3L641/rDHvvVnZAfxaWyr++U/GWRvi/cyk9kkhUX/Gu2/wgJ7EfKSs3F8Nh5tJ4tX6JbLfhymX1sxCPSNTztJ6LGWHElPxHVB4OfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxR/wbJTJfoSPRzAxlAB/UXxh06kw4ifVxAJ60jn0blUTVFaTlPx5pLvDCIfV0J0Wn9TgZhrd4X4sS785y7ntJiaDrXukpruwrKLJHHK5CkO6B4ilMCHGGfjVaYLgl4HfhrHOCXUjJ5BCEm0pdoFFtJwaH/yTPxahnsLhuJP8hpDoTyF2ckyVttPoD3cjfdOasHilkpwnMoVtSWQwiLiTObjLyweLHFfLWYNeii9l21WS6b27bnFpOjfSXXZeXpsX3K9znPoy/ZNqpkU/ZzEUPUwD81W25WxPptiZA21l1vev6ztuN1PSlimRycnj99+LhdjZoXuOgesaTgyqGVQ1WedvpPkzXYWWQ1NqyjFV6GtpKfRdcca7CsMfWDMsU4daCl1n5tWU8MFwoaulTywZNF8Y7moptGjry3W9/S1l21W40ufbt481TzsWa1rbj9hvn3Yc1nXqGhZnZLPOQ1u/eL5+zPr1lOxp33kGFF0Hl31NvXmBbVvXS7/mwX3b7OOv7brKc9SPydqGc87y+mnnTwtreMk/l80gFgn2sLsBkZgarYOe9BAS6+zx6HZRN7yWLSqJ6k+UVMcHERs47O67iKyjJqP2+NYO9CCLrFO6r0ug182GRD50fwdimSzm1LRFWz84P5rshdqyEEXHHiD7rn3W+ZFjGBTretlWDW6OY8yXqUWxeyCGwYv+Mty75v59O+w6vH6MCwvdF0Hm/Lh776LJEfRkkvixL+OhajU8+DeIKM9kfV9pYQ7ZTAyRe9VkvQWDaFnIcwAi9y22LT/QCGpVP/xine3emwLxsBjSgymQFust5fKBi82Rxb1luJFFRgRcm7bttgPBI+tBh9boJgN9aDEPmqCtu632IC/YSz9YqDoND375NC8qiW+KEhARRMqVmEsVi4jcYrktLXuZTrVZbSGy9dpqCL2cQtWbk/VuUQeWr7yc1vARESQO+cBdEtWq7mzbGpwMZlmFkHiqB4MJp61gGmPiobb4B6zZGt/gJ9NdUZI84zYaiRKvNwkM7NZSwgyyN+wx2SBV9L53nXhQYBBjC6Z/0+gXQeErMeu27aAQ2jtFOux+URfBLZ3taxIsXyvTMieRBejnZVURTsYDDYR+Vkampdiuovu1OLU3+Clbe5FLQzXAxjG4Z4Sv9hap8cEvQrw3N4KI27Jsv/bz0kQxPy2f9irFzO7zlWI2/zLBlnHv8ziyA/XddiWh+BAmBrLam4LiFv1yok/INx5xtV4bsp01lPyiCrBvTwbJbWq/xyNI6eclqwgik8i65ySGQIt8KH7YfuXqzHdb+4P3yx4qPUjqTj4A7mjGsTqt8r/wk/VmEThP8Q8/iIKaoOQnokZg8BMZiv+wh8hQLPmJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkM1fBf8ul/c1aNVef3f+8zamxhTz3fpcYW9tLzQ2pscXof2KjGiFYGlvxEhmLwExmKwb9oeVx9fhMyM2qSaIVh8BMZqjmCPz+KVx/chGfdYS+uVurVauYonrl/oz18/2d45xXx/1em1EzN727iz3/5K6R/p6alT97Fml++i7fVpJ9dmnvHsQmvXix1IHK5rUiPAxeeLLcMUXNrfPDLwH9sP+55+Tr63pDDJBLtY0gfG8VttYjP/M/wt3v/Bt88MYvDl8Swcw4/GVTzgv7g89hx9zxGP/5UfQC8/dFv8K0v34Ovq2mPCujwOXUc15E82IGZF58r8SAKYcsL8jiBh8VxP76dvcTSytP44A/twOMi0GKtaloEVviBDjVe7PY//hy/7jmHP3WWb92J74ggLO0uPHT33fj7+Y/wW2v6E7yeuxs7PneXNeUnA1oE/Xe9jh/XbOmEe1hEq0wT1fmnkVGpdurFMfVZsY9u/Bxfvld/px7C2j9UoyV86XMhfGv+33BdTvzu/+Cdu0N46A+sWWXNnlZpv8hI2J5Hq1UTBL8T9I/i/YOTbrpdN1bq/xu8/gnw249F/n735/ElNSvICfoTuZeQlKn/2ZdY8tOq1fjgn3kLF9rtYKum7vz5ex/Br2/ofxWYx63fqNGS7NT/xY9u4vV5lEn5pWnkTnUgcVYE/Qs7sEZ9SrRaNUfaPz6LW2pUtuRXSvvXfOURfHnwUfyDk4/PvIafjKvxMqzUPzeDXaKKUDnlH8Mt8YCwiYyEaT+tYo0P/tansXfXEZxQ9f1n0xuRfPl74oFwHrlSb9Du/jb+4sRf4xd71au+1zbgOz0ItAMEWKk/8K0KKT8QRUzsV766s47jwRNYe/YcHhYPhJmS7x3thklr+dPT6jOilWNV/MOe669sxBv3TuEvAtUG7x/2fIr0r/4Z2PzHSJQp+fkPe8g0TdTaX53bF/daf9jj/g3A/M/wxuAj+OOvVGgv+N1HGF0w5Scyy4oL/jXbf4TvYD9+5PyF35/9HH/80xP4qkjrS3n7X36JNZfy2LF5XYWUn8g8DU/7iagxVlzJT0T1weAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfgbZbIf4e402LM/NUqTBH8e6e4wwmE11BoUN9PoDvdjWfrNWc5tN7HpVBjdI3w0rWZNEPwy8Nsw1jmBXC4nhgmkkERbil1gUXnTKfMeyHUnf7e/oa70FVq6hgvzatLywXChq6WvMGVNTBX6WroKwx9YE5apQy2FrjNyDTmvpdBSNDjLzxeGu+S4vpyzXWkp266SPD9tfXvb6nNx3lNnuornKfPavOB+nePUl+m7omZaAsd/yDtrW2C+ugf+fWqDb33/ur7jdu+dtkzw/gry+P33olrinp6x15o/01fbvSCfhge/9WUr88W0v8xyvFyAKr6HhU4Gv/ySeetbXzp3f0vZdhWswC/zsHAeCs6xWNPafsR++7TjsK6TFkR28HjrF8/Xj1m/npI9HXzY6Iqug8u+pt68wLat66Vf8+C+bfbx13pdvcB38AGweA1P++eyGcQiG9SUYwMiMTVaBz3pISTW2ePR7T3AtewdaGgT1Znjg4gNHHb3XSSWwkQyao9v7UAPssjetCexLoHeuNf/YOj+DsQyWcypaYu2fnB+NNkLtWUhio49QPZd+6zzI8cwKNY9rG2/ajfHMSYqZt66UeweiGHwoj8J9665f9+OaFJW8fRjXFh+5MfA/f41QvHdwCVWABaj4cG/QUR5Juv7SgtzyGZiiNyrJustGETLQp4DELlvEQFmCTSCbktCbM4n1tkOd+viYTGkB5N8m+CsK4b4SfW5IB+42Bzx1q3FjSwymSTatG23HQgeWQ86tqpRQQb60GIeND55jGcjaC96kIbQLh6aDP/aNTz4Q/dFikvim6IERASRciXmUsUiIrdYbkvLXqZTbUiKEnbCagQVw+UUqt6cfEORGBSlr1pXDCOi9HXIB+6SyIzDOS5ncDKYZSOCPJLFuJMZucRDQXxXlnvvq1HjW/tluitKkmfc10qixOtNAgO7tRuaQfaGPZYf6faVYpZ14kGBQYxNqumyptEvgsJXYtZt20Hiy9op0uHEElql3dLZvibB8rUyLXMSWYB+XlYV4WQc/RXOycrIzo/7H8pS0f1aHPkqMVzjK9RSKX6pqgBVpwle9UXRmxtB5ECbSiPt135emijmp3tEEKkUM7vPV4rZ/MuEw91IayWE93kc2YH6bruSUHwIEwNZxK317KHad+fRJ0RJLwLUXq8N2c4aSn5RBdi3J4PkNrXf4xGk9POSVQSRSWTdcxJD4G8rQvHD9itXZ7776jV4v+yh0oOkfkJIxDcgPTJtvepLj/Rj/P7e8m0qVNEq76Lb/huC7FM59Gp1UCJqipKfqDb868P6YPDTihNNTqDjvKh28K9Al2SVp/20mskM4FhEb8OhWjD4iQzFtJ/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMlTDf8yj/81ZNVZ/+//L42psYccP/0SNVef//j//Zet9YKMaI1oZWPITGYrBT2QoBv+i5XH1+U3IzKhJohWGwU9kqOYI/vwoXn1wE551h724WqlPhpmj3rLPj+LqafH/0yV+w/3TPP78rX9C+lM1Ld2+gTVv3cDbatIvj3e+vxHP3O8Nf3ux1IHIUn8r0uPAhSc34dWSyxA1t8YHvwz8x/bjnpevo+8NOUwi0T6G9LFR3FaL+MjlnzyCh53lE7NIn1Lzgu76LHZ88UOMfuJF/9sfX8e3Nt+Nr6tpjwz8r+Enf3gOhy/NWsNzP3gEv/7Bc3hnXi3iCmHLC/I4YR3H49v5u/G08jQ++EM78LgI4lirmhaBFX6gQ40Xu331PGZ2nfOWb91pBWFpd+Ghz38Bf//RJ/itNX0br89+ATs+e5c15RfCV38ogv6Jr6lpYM1XHsGX1TjRatNEdf5pZFQqn3pxTH1W7NZ7Y2hdr/euH8LasBot4Uuf/QK+9a+/w3U58em/4Z0vfgEPlYp9zfVXVNr/Z/vxa/UZ0WrTBMHvBP2jeP/gpJXKJw+WL/lrZqX+1/G6qEP89pMPgc9/Fl9Ss4KcoD/+m5fwnEz9f/oSS35atRof/DNv4UL7S0iKoK+m7rx2fQdm3ptTU1Iet3JqtCQ79X/x4zxe/whlUn5pCr8ZfATf+akI+h9+G2vUp0SrVXOk/eOzuKVGZUt+pbR/zZZOtJ561Hu/PvOa1epeiZX6z17DLiyU8v8cH7kN91P4B6b9tIo1Pvhbn8beXUdwwnl1l96I5MvfEw+E88iVeoMmGwjFfPmKzVk+sUtsxtcOEGCl/sC3KqT8wNfwpyf+Gr/Y67zmO4nP//QcvikeCL/6x5IHYjVMWsdR6jUjUZNbFf+wZ/b0JlxaP1lUbfD+Yc+nSP+zqBu0/BESZUp+/sMeMk0TtfZX5/bFvdYf9rh/A5AfxaVTHWjdUqG94NNPMLpgyk9klhUX/Gu2/wgJ7EfKqSY8dh6tZ0+gXOy/PfcW1lz5EDtaQhVSfiLzNDztJ6LGWHElPxHVB4OfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxR/wbJTJfoSPRzAxlAA7+F7YdCqM+Ek1sZBYite1Ck1S8ueR7g4jHFZDd1p8UoObaXSH+7Es/eYs57abmAy27pGa7sKyiiZzyOUqDRNIxYCetBhn4FelCYJfBn4bxjonvJuIJNpS7AKLaDk1PvgnfyxCPYXDcedZHUKiP4XYyTFV2k6jP9yN9E1rwuKVSnKeyBS2JZHBIOJO5uAuLx8sclwtZw16Kb6UbVdJpvfuusWl6dxId9l5eW1ecL/OcerL9E+qmRb9nMVQ9DANzFfZlrM9mWJnDrSVWd+/ru+43UxJW6ZEJieP338vauC7puZlZXUj6/yNNH+mq9ByaEpNOaYKfS0thb4rznhXYfgDa4Zl6lBLoevMvJoSPhgudLX0iSWD5gvDXS2FFm19ua63v6VsuwpX+nz79rHmacdiTWv7Efvt047Duk5dw+KMbNZ5aOsXz9ePWb+ekj3tO8+Aouvgsq+pNy+wbet66dc8uG+bffy1XdepQ/Y2568MF6bUNfXO2z6u4H6ovIaX/HPZDGKRYA+7GxAR9bd66UkPIbHOHo9u7wGuZWtrU1gUkXUcH0Rs4LC77yKyYSoZtce3dqAHWWSd0n1dAr1uNiTyofs7EMtkMaemLdr6wfnRZC/UloUoOvYA2Xfts86PHMOgWNfLtmpwcxxjvkwtit0DMQxe9Je/3jX379th1+H1Y1zYhgiQ7E2La5VAVF3T0H3iw4yoJobbkMzYn1F1Gh78G0SUZ7K+r7Qwh2wmhsi9arLegkG0LOQ5AJH7Ftv0FGgEtaoffrHOdq9hSzwshvRgClQ39JZy+cDF5sjiGsVuZJGxgs3bdtuB4JH1oGOrGhVkoA8t5kETEIoPYWSz2LdWjZi+KB+wsr3IbvCj6jU8+K0nd7AkvilKQEQQKVdiLlUsInKL5ba07GU6JUoyUcJOOK3Zl1OoenOy3p0YtFu+1fojovR1yAfuksiMwzkuZ3AymGUmHyQTnWPuwyeOkbo8WEzU+AY/me6KkuQZt9FIlHi9SWBgt5YSZpC9YY/JBqmi973rxIMCgxjzNXiVMo1+ERS+ErNu2w4Kob1TpMOJJTRIuaWzfU1qy2q1zElkAfp5WVWEk/FAA6GflZGdH/c/lKWi+7U4S2nwkxnAnX7orEaND34R4r25EUTclmX7tZ/3NBfz0z0iiFSKmd3nK8Vs/mWCLePe53FkRYpYz21XIr+kEwNZ7U1BcYt+OdEn5BuPuFqvDdnOGkp+UQXYtyeD5Da13+MRpPTzklUEkUlk3XMSQ6BFPhQ/bL9ydea7rf3B+2UPlR4k1JxW+V/4yXqzCJyncujV6qBE1BQlPxE1AoOfyFD8hz1EhmLJT2QoBj+RoRj8RIZi8BMZisFPZCgGP5GhGPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBj+RoRj8RIZi8BMZisFPZKiG/5JP/5uzaqyx/vP7/1ONVed05EE1ZtvztX+vxohWBpb8RIZi8BMZisG/aPN4O3kPzv9KTRKtMAx+IkM1R/DnR/Hqg5vwrDvsxdVKvVrNHPWWfX4UV0+L/58u0eubtd3Atqx1j6J0M+PHeO2/7sS/2+kNO9/6WM3TfSxK/a/i1EVgtOse/LdfzKvPiVaOxge/DNDH9uOel6+j7w05TCLRPob0sVHcVov4yOWfPIKHneUTs0ifUvOCQtvQKrY1o0X/7OQRtB7ciY1q2iMD/y/xxMYX8W+vvWYN15/+On52dAivfaQWcX0OX0+9g13bgR3//X385TfvVp8TrRyND/7QDjwugjjWqqYRQviBDjVe7PbV85jZdc5bvnWneFio8SL2tmbevKweJNPInepA65ZSXTp/Djv/SgT9n21W08CX/kMbvq3GiVabJqrzTyOjUvnUi2Pqs2K33htD63q9d/0Q1obVaAlrtnSidXwWt+REfg7vt3ciXCr2NRM/VWn/U0fxM/UZ0WrTBMHvBP2jeP/gpJXKJw+WL/lrZqX+R5CbsbMGPLANa9SsICfoH5p9Gtdl6n/8aZb8tGo1Pvhn3sKF9peQFEH/+PYFimRh7XqRxr83p6akPG7l1GhJdup/YXIUuTdFLaFkyi9dw3j663jluAj6v/oGvqQ+JVqtmiPtd9JyaeZoxbTfSuNPPYqMKMktM68hPa7Gy7DX2Y80Fkr538a7H6pR8TBIMe2nVazxwd/6NPbuOoITzqu79EYkX/6eeCCcR67U6z7ZQCjmX3jSWz6xS2zG1w4QYKX+YpkKKT+wGckf/if84PvOa77/gfuOv4gfiAfCz/5Xqdd9d+OP2jutV317B6fUZ0Qrx6r4hz2zpzfh0vrJCtWGPK4+/xyw7wTKZf38hz1kmiZq7a/O7Yt7rT/scf8GID+KS2Vf3yn5y5hZMOUnMsuKC/4123+EBPYj5VQTHjuP1rPlS3SZFVjL7NtRIeUnMk/D034iaowVV/ITUX0w+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFD8Ac9GmexH+HgEE0MJ8BfFFzadCiN+Uk2U0ZPOoXermqAFNUnJn0e6O4xwWA3dafFJDW6m0R3ux7SarKvl3HYTk8HWPVLTXVhW0WQOuVyJId0DxFKYEOMM/No0QfDLwG/DWOeEuqETSCGJtpRp4UZ0ZzU++Cd/LEI9hcNxJ/kNIdGfQuzkmCptp9Ef7kb6pjVh8UolOU9kCtuSyGAQcSdzcJeXDxY5rpazBr0UX8q2qyTTe3fd4tJ0bqS77Ly8Ni+4X+c49WX6J9VMi37OYih6mAbmq2zL2Z5MsTMH2sqs71/Xd9xupqQtUyKTk8fvvxfVCGSIckgMqnlUM1nnb6T5M12FlkNTasoxVehraSn0XXHGuwrDH1gzLFOHWgpdZ+bVlPDBcKGrpU8sGTRfGO5qKbRo68t1vf0tZdtVuNLn27ePNU87Fmta24/Yb592HNZ16hoWZ2SzzkNbv3i+fsz69ZTsad95BhRdB5d9Tb15gW1b10u/5sF92+zjr+26Bs/RIq9b8DOqSsNL/rlsBrFIsIfdDYjE1Ggd9KSHkFhnj0e3izritWxRSVR/opQ6PojYwGF330VkXTUZtce3dqAHWWSd0n1dAr1uNiTyofs7EMtkMaemLdr6wfnRZC/UloUoOvYA2Xfts86PHMOgWNfLtmpwcxxjvkwtit0DMQxe9Jfh3jX379th1+H1Y1yY9V3pbGcDaZ00PPg3iCjPZH1faWEO2UwMkXvVZL0Fg2hZyHMAIvct9qsaSHGt6oefLxDEw2JID6ZAdUNvKZdBhM2RxQXRjSwymSTatG23HQgeWQ86tMY3GehDi3nQBJT+rtBiNTz4Q/dFikvim6IERASRciXmUsUiIrdYbkvLXqZTbVZbiGzFthpCL6dQ9eZkvVvUheWrL6dVfESUvg4ZREuiWtedbVuDk8EsIyu7OXmstjYXKqvxDX4y3RUlyTNuo5Eo8XqTwMBuLSXMIHvDHpMNUkXve9eJBwUGMeZr8CplGv0iKPypY722HRRCe6dIhxNLeE3ols72NQmWr5VpmZPIAvTzsoMoHmgg9LNK2fPj/oeyVHS/FmdRDX4iuzk8ACS3LeGakqvxwS9CvDc3gojbsmy/9vPSRDE/3SOCSKWY2X2+UszmXybYMu59Hkd2oL7briQUH8LEQFZ7U1Dcol9O9An5xiOu1mtDtrOGkl8Eyb49GREkar/HI0jp5yWrCCKTyLrnJIZAi3wofth+5erMd1v7g/fLHio9SOrJvaZ8Fbxkq/wv/GS9WQTOU/wDEKKgJij5iagRGPxEhuI/7CEyFEt+IkMx+IkMxeAnMhSDn8hQDH4iQzH4iQzF4CcyFIOfyFAMfiJDMfiJDMXgJzIUg5/IUAx+IkMx+IkMxeAnMhSDn8hQDH4iQzX8l3z635xVYyvL7//eZ9SY7cA3lr8nAKJ6YslPZCgGP5GhGPyLlsc739+If5hRk0QrDIOfyFDNEfz5Ubz64CY86w57cbVSr1YzR71lnx/F1dPi/6dLdN9kbTewLWvdoyjdzJjH1eedY7CHVy+WOhBZ6n8NPxkHfrF3I/625DJEza3xwS8D9LH9uOfl6+h7Qw6TSLSPIX1sFLfVIj5y+SeP4GFn+cQs0qfUvKDQNrSKbc1o0T87eQStB3dio5r2yMDfinT4nDqO60ge7MDMi8+VeBCF8NUfTuE77cA3T8ziL7Yvvftpojut8cEf2oHHRaDFWtW0CKzwAx1qvNjtq+cxs+uct3zrTvGwUONF7G3NvHlZPUimkTvVgdYtpYI1hC0viKD/rtc38JotnXAPi2iVaaI6/zQyKtVOvTimPit2670xtK7X36mHsDasRkuwAnh8FrfkRH4O77d3IrxAQT0rqxHyWERGwvY8Wq2aIPidoH8U7x+cdNPturFS/yPIiSiWWQMe2IY1alaQE/Qnci8hKVP/sy+x5KdVq/HBP/MWLrTbwfZ4FXXntetFGv/enJqS8riVU6Ml2an/hclR5N4UtYSSKb9kVwkSZ0XQv7Cj7AOCaLVojrTfSculmaMV034rjT/1KDJOPj7zGtLjarwMe539SGOhlH8Mt+bVqMxImPbTKtb44G99Gnt3HcEJVd9/Nr0RyZe/Jx4I55Er9QZNNhCK+Ree9JZP7BKb8bUDBFipv1imQsoPRBHTt/vgCaw9ew4PiweC/rbAE8IfPvSI9arvmVem1GdEK8eq+Ic9sq5+af1khWqDfI33HLDvBMpm/TXiP+yhla6JWvurc/viXusPe9y/AciP4lLZ13dK/rJI3xdu5ScyyQos+dUf47j1fNlIV75El1nBCashr36lvsSSn1a6hgc/ETXGikv7iag+GPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBj+RoRj8RIZi8BMZisFPZCgGP5GhGPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBn+jTPYj3J0Ge/anRmmS4M8j3R1GOKyGWoPiZhrd4X5Mq8m6Ws5tN7HpVBjdI3w0rWZNEPwy8Nsw1jmBXC4nhgmkkERbyrRwI7qzGh/8kz8WoZ7C4bjTo0YIif4UYifHVGk7jf5wN9I3rQmLVyrJeSJT2JZEBoOIO5mDu7x8sMhxtZw16KX4UrZdJZneu+sWl6ZzI91l5+W1ecH9OsepL9M/qWZa9HMWQ9HDNDBfZVvO9uIngcyBtjLr+9f1HbebKWnLlMjk5PH77wXdcbLTjkaaP9NVaDk0paYcU4W+lpZC3xVnvKsw/IE1wzJ1qKXQdWZeTQkfDBe6WvrEkkHzheGulkKLtr5c19vfUrZdhSt9vn37WPO0Y7Gmtf2I/fZpx2Fdp65hcUY26zy09Yvn68esX0/JnvadZ0DRdXDZ19SbF9i2db30ax7ct80+/kVeV6qLhpf8c9kMYpFgV1cbEImp0TroSQ8hsc4ej27vAa5la2tTWBSRdRwfRGzgsLvvIrEUJpJRe3xrB3qQRdYp3dcl0OtmQyIfur8DsUwWc2raoq0fnB9N9kJtWYiiYw+Qfdc+6/zIMQyKdb1sqwY3xzHmy9Si2D0Qw+BFfxnuXXP/vh3RpKzi6cdId1rDg3+DiPJM1veVFuaQzcQQuVdN1lswiJaFPAcgct9iOwgMNIJa1Q+/WGe7qCQp4mExpAdToLoh03iHfOBic8RbtxY3sshkkmjTtt12IHhkPejYqkYFGehDi3nQ0LJqePCH7osUl8Q3RQmICCLlSsylikVEbrHclpa9TKfarLaQCasRVAyXU6h6c7LenRgUpa9aVwwjovR1yAfuksiMwzkuZ3AyGFoxGt/gJ9NdUZI84zYaiRKvNwkM7NZSwgyyN+wx2SCll2KWdeJBgUGM+Rq8SplGvwgKX4lZt20HhdDeKdLhxBIatdzS2b4mwfK1Mi1zElmAfl5WFeFkPNBA6GdlZOfH/Q9lqeh+LQ4b/Bqv8cEvQrw3N4KI27Jsv/bz0kQxP90jgkilmNl9vlLM5l8m2DLufR5HdqC+264kFB/CxEBWe1NQ3KJfTvQJ+cYjrtZrQ7azhpJfVAH27ckguU3t93gEKf28ZBVBZBJZ95zEEGiRD8UP269cnflua3/wftlDpQcJNadV3kW3rDeLwHkqh16tDkpETVHyE1EjMPiJDLXK034iKoclP5GhGPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBj+RoRj8RIZi8BMZisFPZCgGP5GhGPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBj+RoRj8RIZi8BMZisFPZCgGP5GhGPxEhmLwExmKwU9kKAY/kaEY/ESGYvATGYrBT2QoBj+RoRj8RIb6TEFQ4w3R/+asGlvZeh/YqMaIVgaW/ESGYvATGYrBv2h5XH1+EzIzapJohWHwExmqOYI/P4pXH9yEZ91hL67m1bxSZo56yz4/iqunxf9PT6uZGmu7gW1Z6x5F6WZGuzT3jmMTXr1Y6kDkcluRHgcuPFluGaLm1vjglwH62H7c8/J19L0hh0kk2seQPjaK22oRH7n8k0fwsLN8YhbpU2peUGgbWsW2ZrTon508gtaDO1HcNq8COnxOHcd1JA92YObF50o8iELY8oI8TljH8fj2kPqcaOVofPCHduBxEWixVjUtAiv8QIcaL3b76nnM7DrnLd+60wrC0uxtzbx5WT1IppE71YHWLaWCVQa0CPrvRtU0sGZLJ9zDIlplmqjOP42MSrVTL46pz4rdem8Mres3qCkphLVhNVqCFcDjs7glJ/JzeL+9E+EFCupZWY2QxyIyErbn0WrVBMHvBP2jeP/gpJtu142V+h9BTkSxzBrwwDasUbOCnKA/kXsJSZn6n32JJT+tWo0P/pm3cKHdDrZq6s5r14s0/r05NSXlcSunRkuyU/8Lk6PIvSlqCSVTfsmuEiTOiqB/YUfZBwTRatEcab+TlkszRyum/VYaf+pR7/36zGtWq3sl9jr7kcZCKf8Ybs2rUZmRMO2nVazxwd/6NPbuOoITqr7/bHojki9/TzwQziNX6g2abCAU8+UrNmf5xC6xGV87QICV+otlKqT8QBQxfbsPnsDas+fwsHgg6G8LPCqjkMuXes1I1ORWxT/skXX1S+snK1Qb5Gu854B9J1A2618i/sMeWmmaqLW/Orcv7rX+sMf9G4D8KC6VfX2n5C+L9H3hVn4ik6zAkt/76zqbbKQrX6LLrOCE1ZC3fKW+xJKfVpqGBz8RNcaKS/uJqD4Y/ESGYvATGYrBT2QoBj+RoRj8RIZi8BMZisFPZCgGP5GhGPxEhmLwExkJ+P9ahST1rM1NIAAAAABJRU5ErkJggg==)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:23.284019Z",
          "iopub.execute_input": "2021-05-20T10:54:23.284509Z",
          "iopub.status.idle": "2021-05-20T10:54:23.301584Z",
          "shell.execute_reply.started": "2021-05-20T10:54:23.284463Z",
          "shell.execute_reply": "2021-05-20T10:54:23.300511Z"
        },
        "trusted": true,
        "id": "zrRasWijQtjt",
        "cellView": "form"
      },
      "source": [
        "#@title Code to statically visualise the LSTM activations.\n",
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "\tif s == ' ':\n",
        "\t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\t\n",
        "# print html\n",
        "def print_color(t):\n",
        "\tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_clr(value):\n",
        "\tcolors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
        "\t\t'#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "\t\t'#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "\t\t'#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "\n",
        "def visualize(input_word, output_word, att_w):\n",
        "  for i in range(len(output_word)):\n",
        "    print(\"\\nOutput character:\", output_word[i], \"\\n\")\n",
        "    text_colours = []\n",
        "    for j in range(len(att_w[i])):\n",
        "      text = (input_word[j], get_clr(att_w[i][j]))\n",
        "      text_colours.append(text)\n",
        "    print_color(text_colours)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT7TvyNc-Rlc"
      },
      "source": [
        "`connectivity()`: Helps to visualise what the sequence to sequence model learns with the help of attention network.\n",
        "#### Parameters:\n",
        "**input_words**: Accepts string as input. Here we pass the transliterated roman word\n",
        "\n",
        "**rnn_type**: Accepts string as input. Here we pass the type of RNN being used. THe acceptable values are 'RNN', 'LSTM', and 'GRU'.\n",
        "\n",
        "**file_path**: Accepts a string as input. Here we pass the file location where we want to store the connectivity visualisation. The visualisation is stored with the file name: `connectivity.html` \n",
        "\n",
        "`create_file()` is used to create and store the connectivity.html file in the specified  location.\n",
        "\n",
        "#### Parameters:\n",
        "**text_colors**: List of list where each sublist denotes the color to be given to every input character on mouse hover action on an output character.\n",
        "\n",
        "**input_words**: Accepts string as input. Here we pass the transliterated roman word.\n",
        "\n",
        "**output_word**: Accepts string as input. Here we pass the predicted output word\n",
        "\n",
        "**file_path**: Accepts a string as input. Here we pass the file location where we want to store the connectivity visualisation. If not specified the default file path is set to the current working directory. The visualisation is stored with the file name: `connectivity.html` \n",
        "\n",
        "`get_shade_color(value)`: Returns a specific colour depending the value passed to it. Here the parameter `value` Expected a floating number between 0-1 denoting the attention weight of the j<sup>th</sup> input character while predicting the i<sup>th</sup>output character.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:23.907044Z",
          "iopub.execute_input": "2021-05-20T10:54:23.907370Z",
          "iopub.status.idle": "2021-05-20T10:54:23.926100Z",
          "shell.execute_reply.started": "2021-05-20T10:54:23.907339Z",
          "shell.execute_reply": "2021-05-20T10:54:23.924813Z"
        },
        "trusted": true,
        "id": "WTNepLwIQtju",
        "cellView": "form"
      },
      "source": [
        "#@title Code for connectivity visualisation.\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_shade_color(value):\n",
        "\tcolors = ['#00fa00', '#00f500',  '#00eb00', '#00e000',  '#00db00',  \n",
        "           '#00d100',  '#00c700',  '#00c200', '#00b800',  '#00ad00',  \n",
        "           '#00a800',  '#009e00',  '#009400', '#008f00',  '#008500',\n",
        "           '#007500',  '#007000',  '#006600', '#006100',  '#005c00',  \n",
        "           '#005200',  '#004d00',  '#004700', '#003d00',  '#003800',  \n",
        "           '#003300',  '#002900',  '#002400',  '#001f00',  '#001400']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "def create_file(text_colors,input_word,output_word,file_path=os.getcwd()):\n",
        "  text = '''\n",
        "  <!DOCTYPE html>\n",
        "  <html>\n",
        "  <head>\n",
        "    <meta charset=\"UTF-8\"> \n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script>\n",
        "            $(document).ready(function(){\n",
        "            var col =['''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word)):\n",
        "              text=text+'''['''\n",
        "              for j in range(len(text_colors[k][i])-1):\n",
        "                text=text+'''\\\"'''+text_colors[k][i][j]+'''\\\"'''+''','''\n",
        "              text=text+'''\\\"'''+text_colors[k][i][len(text_colors[k][i])-1]+'''\\\"'''+'''],'''\n",
        "  text=text[0:-1]\n",
        "  text=text+'''];\\n'''\n",
        "  \n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseover(function(){\\n'''\n",
        "            for j in range(len(input_word[k])):\n",
        "                       text=text+'''$(\\\".t'''+str(k)+str(j)+'''\\\").css(\\\"background-color\\\", col['''+str(i)+''']'''+'''['''+str(j)+''']);\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseout(function(){\\n'''\n",
        "            for l in range(3):\n",
        "              for j in range(len(input_word[l])):\n",
        "                text=text+'''$(\\\".t'''+str(l)+str(j)+'''\\\").css(\\\"background-color\\\", \\\"#ffff99\\\");\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "  text=text+'''});\\n\n",
        "</script>\n",
        "  </head>\n",
        "      <body>\n",
        "          <h1>Connectivity:</h1>\n",
        "          <p> The connection strength between the target for the selected character and the input characters is highlighted in green (reset). Hover over the text to change the selected character.</p>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <p>\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>\n",
        "          '''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''\\n'''+'''\\t'''+'''<div class=\"h'''+str(k)+str(i)+'''\\\">'''+output_word[k][i]+'''</div>'''\n",
        "      text=text+'''</div>'''+'\\n'+'\\t'+'''<div>  </p>'''+'\\n'+'\\t'+'''<p>\n",
        "      <div> Input: </div>\n",
        "      <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''    \n",
        "      for j in range(len(input_word[k])):\n",
        "        text=text+'''\\n'''+'''\\t'''+'''<div class=\"t'''+str(k)+str(j)+'''\\\">'''+input_word[k][j]+'''</div>'''\n",
        "      if k<2:\n",
        "          text = text+'''</div></p></div><p></p></div>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''\n",
        "  text=text+'''\n",
        "        </div>\n",
        "        </p>\n",
        "        </div>\n",
        "        </body>\n",
        "  </html>\n",
        "  '''\n",
        "  fname = os.path.join(file_path,\"connectivity.html\")\n",
        "  file = open(fname,\"w\")\n",
        "  file.write(text)\n",
        "  file.close()\n",
        "\n",
        "def connectivity(input_words,rnn_type,file_path):\n",
        "  color_list=[]\n",
        "  input_word_list=[]\n",
        "  output_word_list=[]\n",
        "  for k in range(3):\n",
        "    output_word, input_word, _ ,att_w = inference_model(input_words[k],rnn_type)\n",
        "    text_colours=[]\n",
        "    for i in range(len(output_word)):\n",
        "      colour=[]\n",
        "      for j in range(len(att_w[i])):\n",
        "        value=get_shade_color(att_w[i][j])\n",
        "        colour.append(value)\n",
        "      text_colours.append(colour)\n",
        "    color_list.append(text_colours)\n",
        "    input_word_list.append(input_word)\n",
        "    output_word_list.append(output_word)\n",
        "  create_file(color_list,input_word_list,output_word_list,file_path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGFRPgAzBld9"
      },
      "source": [
        "`transliterate()`: Finds the predicted target word for a given tansliterated roman word, plots the attention heatmap and visualises the LSTM activations if the visual_flag is set to True.\n",
        "#### Parameters:\n",
        "input_word:Accepts string as input. Here we pass the transliterated roman word</br>\n",
        "**rnn_type**: Accepts string as input. Here we pass the type of RNN being used. THe acceptable values are 'RNN', 'LSTM', and 'GRU'.</br>\n",
        "**file_path**: Accepts a string as input. Here we pass the file location where we want to store the attention heatmap for the input word and the predicted target word. If not specified the attention heatmaps is stored in the current working directory by the name \"attention_heatmap.png\"</br>\n",
        "**visual_flag**: Accepts a boolean True or boolean False. If the visual_flag is set to true then the code to statically visualise the LSTM activations are called. Default value of the flag is set to \"True\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:24.526229Z",
          "iopub.execute_input": "2021-05-20T10:54:24.526584Z",
          "iopub.status.idle": "2021-05-20T10:54:24.532481Z",
          "shell.execute_reply.started": "2021-05-20T10:54:24.526554Z",
          "shell.execute_reply": "2021-05-20T10:54:24.531248Z"
        },
        "trusted": true,
        "id": "D9WCdzS8Qtjv",
        "cellView": "form"
      },
      "source": [
        "#@title Code to get the predicted target word from a transliterated roman word.\n",
        "def transliterate(input_word,rnn_type,file_name=os.path.join(os.getcwd(),\"attention_heatmap.png\"),visual_flag=True):\n",
        "  predicted_word, input_word, attention_plot,att_w = inference_model(input_word,rnn_type)\n",
        "\n",
        "  print(\"\\n\",'Input:', input_word)\n",
        "  print('Predicted transliteration:', predicted_word)\n",
        "\n",
        "  attention_plot = attention_plot[:len(predicted_word),\n",
        "                                  :len(input_word)]\n",
        "  plot_attention(attention_plot, input_word, predicted_word, file_name)\n",
        "\n",
        "  if visual_flag == True:\n",
        "    visualize(input_word, predicted_word, att_w)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPOpIS-YF2WP"
      },
      "source": [
        "`generate_inputs()`: Randomly chooses 10 inputs from the test dataset and calles the transliteration() to produce the predicted target input and heatmaps. It also set the visual_flag in transliteration() to True only for the first test input and False for the rest 9 test inputs.\n",
        "####Parameters:\n",
        "**rnn_type**: Accepts string as input. Here we pass the type of RNN being used. THe acceptable values are 'RNN', 'LSTM', and 'GRU'.</br>\n",
        "**n_test_samples**: Accepts an integer as input. Here we pass number of test inputs to be used for the heatmap generation. Default value is set to 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:25.325362Z",
          "iopub.execute_input": "2021-05-20T10:54:25.325991Z",
          "iopub.status.idle": "2021-05-20T10:54:25.333592Z",
          "shell.execute_reply.started": "2021-05-20T10:54:25.325936Z",
          "shell.execute_reply": "2021-05-20T10:54:25.332809Z"
        },
        "trusted": true,
        "id": "Ts9Lrr2dQtjv",
        "cellView": "form"
      },
      "source": [
        "#@title Code to randomly choose 10 test inputs for attention heatmap generation.\n",
        "def generate_inputs(rnn_type,n_test_samples=10):\n",
        "  target_words, input_words = create_dataset(test_file_path)\n",
        "  \n",
        "  for i in range (n_test_samples):\n",
        "    index = random.randint(0,len(input_words))\n",
        "    input_word=input_words[index]\n",
        "    file_name=os.path.join(os.getcwd(),\"predictions_attention\",str(run_name),input_word+\".png\")\n",
        "    \n",
        "    if i == 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,True)\n",
        "    elif i > 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jty-fdgbG_YK"
      },
      "source": [
        "Uncomment wandb.agent() to use wandb and comment the call to train()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T10:54:26.216163Z",
          "iopub.execute_input": "2021-05-20T10:54:26.216493Z",
          "iopub.status.idle": "2021-05-20T14:47:39.833648Z",
          "shell.execute_reply.started": "2021-05-20T10:54:26.216462Z",
          "shell.execute_reply": "2021-05-20T14:47:39.832495Z"
        },
        "trusted": true,
        "id": "qVrazjKXQtjv"
      },
      "source": [
        "#wandb.agent(\"utsavdey/attention-seq-to-seq/w8pglu2i\",train)\n",
        "train(use_wandb=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43kCAzvNpz2g",
        "cellView": "form"
      },
      "source": [
        "#@title Download a copy of the predictions_attention and training_checkpoints folder.\n",
        "!zip -r /content/predictions_attention.zip /content/predictions_attention\n",
        "!zip -r /content/training_checkpoints.zip /content/training_checkpoints\n",
        "from google.colab import files\n",
        "files.download(\"/content/predictions_attention.zip\")\n",
        "files.download(\"/content/training_checkpoints.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}